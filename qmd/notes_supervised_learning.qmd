---
title: "Notes supervised learning"
format: html
editor: source
editor_options: 
  chunk_output_type: console
---


## Linear regression

### Gradient descent

* random starting parameters
* select cost function (e.g. MSE, RMSE, ...)
* change parameters to reduce cost function $J$ to possible (local) minimum
* example of gradient descent for cost function $J(w, b)$:
  * for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)$ 
  * for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
  * with $\alpha$: learning rate, e.g. 0.01, corresponds to the step size
  * derivative indicates direction and grade of descent per parameter
  * in case of one parameter, partial derivative is tangent line
  * in case of two parameters, partial derivatives form a 2D-plane 
* parameters are updated simultaneously
* near local minimum update steps get smaller by design, even with constant 
learning rate $\alpha$
* repeat until convergence, i.e. changes get smaller per step
* small leaning rate leads to slow descent (many steps)
* large leaning rate may miss the minimum and fail to converge
* stopping criteria:
  * derivative of cost function is close to zero
  * small changes of parameters 
  
Example with linear regression:

* partial derivatives
* always one global minimum
* in linear regression **batch** gradient descent is used which uses the entire 
training data at each step

In linear regression parameters $w$ can be estimated using the normal equation 
without iterations. This is usually impossible in other models. 


### Feature scaling

Features varying largely in scale will cause parameters $w$ to also be very 
different. This can make gradient descent inefficient as the hyperplane gets 
narrow in one direction. 

Methods of scaling:  

* normalize to maximum: x / max(x)
* normalize to mean and range: (x - mean(x)) / range(x)
* normalize to mean and sd: (x - mean(x)) / sd(x)


### Convergence of gradient descent

* plot cost function $J$ against number of iterations
* ideally $J$ should decrease on every iteration and at some stage flatten out 
(convergence)
* $J$ should never increase as iterations increase, i.e. no bumps, no steady 
increase
* number of iterations will vary based on data and model
* declaration of convergence by setting a minimum $\epsilon$, which sets a 
threshold to the decrease of $J$ for one more iteration. I.e. if $J$ is 
decreasing with less than $\epsilon$, convergence is assumed. 


### Learning rate

* small $\alpha$: many iterations
* large $\alpha$: overshooting, i.e. no convergence (ups and downs in the cost 
function plot)
* good practice: test different learning rates (0.001, 0.003, 0.01, 0.03, 0.1, ...) 
with a small number of iterations and plot cost function versus iterations, e.g. 


### Feature engineering

* transforming or combining original features using intuition, prior knowledge
* polynomial regression to create features from squares or cubes
* feature scaling may become even more important on combined features


## Classification with logistic regression

* sigmoid function: $g(z)=\frac{1}{1+e^{-z}}$
* $g(z)$ is between 0 and 1
* logistic regression applies the sigmoid function to the linear regression 
through the linear predictor $z=wx+b$
* output means $P(y=1|x;w,b)$

```{r}
#| label: sigmoid function

z = seq.int(-5L, to = 5L, length.out = 100)
g = 1 / (1 + exp(-z))
plot(z, g, type = "l")
abline(v = 0, lty = 2)
abline(h = 0.5, lty = 2)
```


### Decision boundary

* decision boundary is given for $z=0$ or $wx+b=0$ (univariate)
* using a decision boundary of 0.5 for the sigmoid function corresponds to 
negative values of $z$ being mapped to 0 and positive values of $z$ being 
mapped to 1
* non-linear decision boundaries can be identified e.g. with polynomials

## Snippets

```{python}
import numpy as np

x = np.random.normal(size=100, loc=1, scale=2)
```

```{r}
mse = function(x, y, w, b){
  p = x * w + b
  se = sum((p - y)^2)
  mse = se / 2 / length(p)
  return(mse)
}

x = 1:100
y = 0.35 * x + rnorm(100, mean = 0, sd = 8) 
y = y + 24.85

plot(x, y, pch = 19, ylim = c(0, 100))

w = runif(100, -5, 5)
b = 5
b = runif(100, -50, 50)

cost = Map(
  mse
  , x = list(x)
  , y = list(y)
  , w = w
  , b = b
) |> 
  unlist()

cost = cost[order(w)]
w = w[order(w)]

plot(w, cost, type = "l")
```
