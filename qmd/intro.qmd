---
title: "Untitled"
format: html
editor: source
editor_options: 
  chunk_output_type: console
---

## Machine Learning

Field of study that gives computers the ability to learn without being 
explicitly programmed. - *Arthur Samuel (1959)*


### Unsupervised learning
 
* clustering
* anomaly detection
* dimensionality reduction


### Supervised learning

* regression (model, cost function)
* gradient descent


#### Gradient descent

* random starting parameters
* select cost function (e.g. MSE, RMSE, ...)
* change parameters to reduce cost function $J$ to possible (local) minimum
* example of gradient descent for cost function $J(w, b)$:
  * for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)$ 
  * for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
  * with $\alpha$: learning rate, e.g. 0.01, corresponds to the step size
  * derivative indicates direction and grade of descent per parameter
  * in case of one parameter, partial derivative is tangent line
  * in case of two parameters, partial derivatives form a 2D-plane 
* parameters are updated simultaneously
* near local minimum update steps get smaller by design, even with constant 
learning rate $\alpha$
* repeat until convergence, i.e. changes get smaller per step
* small leaning rate leads to slow descent (many steps)
* large leaning rate may miss the minimum and fail to converge
* stopping criteria:
  * derivative of cost function is close to zero
  * small changes of parameters 
  
Example with linear regression:

* partial derivatives
* always one global minimum
* in linear regression **batch** gradient descent is used which uses the entire 
training data at each step

In linear regression parameters $w$ can be estimated using the normal equation 
without iterations. This is usually impossible in other models. 


#### Feature scaling

Features varying largely in scale will cause parameters $w$ to also be very 
different. This can make gradient descent inefficient as the hyperplane gets 
narrow in one direction. 

Methods of scaling:  

* normalize to maximum: x / max(x)
* normalize to mean and range: (x - mean(x)) / range(x)
* normalize to mean and sd: (x - mean(x)) / sd(x)


#### Convergence of gradient descent

* plot cost function $J$ against number of iterations
* ideally $J$ should decrease on every iteration and at some stage flatten out 
(convergence)
* $J$ should never increase as iterations increase, i.e. no bumps, no steady 
increase
* number of iterations will vary based on data and model
* declaration of convergence by setting a minimum $\epsilon$, which sets a 
threshold to the decrease of $J$ for one more iteration. I.e. if $J$ is 
decreasing with less than $\epsilon$, convergence is assumed. 


#### Choice of learning rate

* small $\alpha$: many iterations
* large $\alpha$: overshooting, i.e. no convergence (ups and downs in the cost 
function plot)
* good practice: test different learning rates (0.001, 0.003, 0.01, 0.03, 0.1, ...) 
with a small number of iterations and plot cost function versus iterations, e.g. 


## Python snippets

* numpy

```{python}
import numpy as np

## integer array 
np.arange(10)

## float array
np.arrange(10.)

## zeros
np.zeros(10)

```

* plotting

```{python}
#| label: plot-numpy
import numpy as np
import matplotlib.pyplot as plt

x = np.array([3, 200])
y = np.array([200, 500])

plt.scatter(x, y=y, label='sample')
plt.xlabel('size')
plt.ylabel('price')
plt.legend()
plt.show()
```

* vectorization

```{python}
import numpy as np

b = 1
w = np.array([2, 3, 4])
x = np.array([10, 20, 30])

f = 0
for j in range(len(w)):
  f = f + w[j] * x[j]

f + b

np.dot(w, x) + b
```




