---
title: "Notes unsupervised learning, recommenders, reinforcement learning"
format: html
editor: source
editor_options: 
  chunk_output_type: console
engine: knitr
---

```{r}
#| label: setup
#| include: false

knitr::opts_knit$set(
  root.dir = '~/Documents/repos/github/machinelearning'
)
```


## Unsupervised learning

* clustering
* anomaly detection

### Clustering: k-means

* process overview:
  * find n clusters
  * random initial guess of $K$ cluster centroids
  * iteratively assign observations to centroids and update centroids
  * stopping criteria: number of iterations, location of centroids, number of 
  observations changing cluster
* distance is mathematically called L2 norm and written as $||x^i-\mu_k||$
* assigning points to centroids: $min_k||x^i-\mu_k||^2$
* move cluster centroids: $\mu_k:=mean(points_k)$
* corner case: delete cluster of no points assigned to it 
* cost function: $J(c^1, ..., c^m, \mu_1, ..., \mu_K)=\frac{1}{m}\Sigma_{i=1}^m||x^i-\mu_k||^2$, where 
  * $c^i$: index of cluster to which $x^i$ is currently assigned
  * $\mu_k$: cluster centroid k
  * $\mu_{c^i}$: cluster centroid of cluster to which $x^i$ has been assigned
* random initialization:
  * choose $K<m$ ($m$ is number of training samples)
  * randomly pick $K$ training examples
  * set $\mu_1, ..., \mu_K$ equal to these examples
* perform random initialization multiple times
* pick from results using cost function $J$
* choosing $k$:
  * Elbow method: cost function $J$ as a function of $K$. Downside: right $K$ is ambiguous
  * Purpose method: Evaluate $K$ based on later (downstream) purpose, recommended

```{python}
import numpy as np
import matplotlib.pyplot as plt

X = np.random.uniform(size = 1000)
X = X.reshape((250, 4))

centroids = np.array([
  [0.2, 0.2, 0.2, 0.2]
  , [0.6, 0.6, 0.6, 0.6]
])

plt.close()
plt.scatter(X[:, 0], y = X[:, 1], s = 5)
plt.scatter(centroids[:, 0], y = centroids[:, 1], c = 'red', s = 50)
plt.show()

def get_closest_centroid(X, centroids):
  
  K = centroids.shape[0]
  m = X.shape[0]
  dm = np.zeros((m, K))

  for i in range(K):
    d = X - centroids[i]
    d = abs(d)**2
    d = np.sum(d, axis = 1)
    dm[:, i] = d
    
  idx = np.argmin(dm, axis = 1)
    
  return idx


def compute_centroids(X, idx, K):

  m, n = X.shape
  centroids = np.zeros((K, n))
  
  for i in range(K):
    center = np.mean(X[idx == i,:], axis = 0)
    centroids[i, :] = center
  
  return centroids
```


### Anomaly detection

* density estimation, i.e. model probability of features $X$ from a number of 
observations, i.e. define values of $x_1, x_2, ...$ with low and high probability
* define level of acceptance $\epsilon$, identify new observations where 
$p(x_{new})<\epsilon$
* frequently used for/in 
  * fraud detection, e.g. by web activity features or financial transaction features
  * manufacturing, e.g. by using product features
  * CPU monitoring in a data center, e.g. by using instance features
* use Gaussian distribution with mean $\mu$ and variance $\sigma^2$
* parameter estimation $\mu, \sigma$ from sample
* density estimation: calculate probability of a specific feature set, which is 
the product of the individual probabilities, e.g. 
$p(X)=p(x_1; \mu_1, \sigma_1)*p(x_2; \mu_2, \sigma_2)* ... *p(x_n; \mu_n, \sigma_n)=\Pi_{j=1}^{n}p(x_j; \mu_j, \sigma_j)$

Approach:
* choose features that are indicative of anomalies
* fit parameters $\mu$ and $\sigma$
* given a new example, compute $p(x)=\Pi_{j=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_j}e^{-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}}$
* anomaly if $p(x)<\epsilon$

Evaluate anomaly detection system:
* take a number of anomalous ($y=1$) and non-anomalous ($y=0$) observations 
* typically $y=1$ will be (very) small
* split data into training (mostly $y=0$), cross-validation and test set (both mixed)
* train algorithm on training set
* tune algorithm based on test set
* evaluate algorithm based on test set
* alternative in case of very few anomalies: skip test set
* predict $y$ and evaluate, use e.g. precision and recall or F_1-score

Anomaly detection versus supervised learning:
* anomaly detection: very small number of positive examples and very large number 
of negative examples, many different types of anomalies not seen earlier
* supervised learning: large number of positive and negative examples, future 
positive examples will be similar to the ones observed

Feature building/selection:
* verify that features are Gaussian or transform features to be Gaussian
* common problem: $p(x)$ is similar for normal and anomalous examples
* solution strategies: find new features (e.g. by combining existing features)


```{python}

X = np.random.uniform(size = 1000)
X = X.reshape((500, 2))


def estimate_gaussian(X): 
    
    mu = np.mean(X, axis = 0)
    var = np.var(X, axis = 0)

    return mu, var
  

p_val = np.random.uniform(size = 100)
y_val = np.random.randint(0, high = 2, size = 100)
  
def select_threshold(y_val, p_val): 

    best_epsilon = 0
    best_F1 = 0
    F1 = 0
    
    step_size = (max(p_val) - min(p_val)) / 1000
    
    for epsilon in np.arange(min(p_val), max(p_val), step_size):
      
        y_prd = np.int8(p_val < epsilon)
        prec = sum(y_val * y_prd) / sum(y_prd)
        rec = sum(y_val * y_prd) / sum(y_val)
        F1 = 2 * prec * rec / (prec + rec)
        
        if F1 > best_F1:
            best_F1 = F1
            best_epsilon = epsilon
        
    return best_epsilon, best_F1
```


## Recommender systems

### Collaborative filtering

* collaborative filtering in case of item features available
  * predict rating from item features by user
  * minimize cost for all users simultaneously 
* collaborative filtering in case of item features not available
  * assumption: model parameters are available
  * guess set of feature values per record using a cost function
  * $j$ users, $i$ examples (e.g. movies)
  * features for movie $i$ can be obtained by minimizing: $J(x^i)=\frac{1}{2}\Sigma_{j:r(i,j)=1}(w^j*x^i+b^j-y^{(i, j)})^2+\frac{\lambda}{2}\Sigma_{k=1}^n(x_k^i)^2$
  * sum over all movies: $\Sigma_{i=1}^{n}(J(x^i))$
  * put together cost function to learn parameters and features
  * cost function is $J(w, b, x)$
* implementation recommendations:
  * mean normalization, i.e. normalize per row
  * gradient descent in tensorflow (auto-diff), finds derivative automatically 
  by providing cost function
* finding related items
  * features $x^i$ are quite hard to interpret (obtained from minimizing $J$)
  * still, features contain some information on the items
  * i.e. items can be ranked with regards to similarity (distance)
* issues with collaborative filtering:
  * cold start problem (ranking new items with few ratings, new users)
  * use side information on items and users, e.g. genre, movie stars for movies, 
  demographics for users
  

### Content-based filtering

* collaborative filtering: recommendations based on ratings of users with similar 
ratings, i.e. ratings are required
* content-based filtering: recommendations based on features of users and items, 
i.e. features are available, some ratings may be available
* user features: demographics, behavior, preferences, ratings. Feature vector 
$x_u^j$ for user $j$.
* movie features: year, genre, reviews, average ratings. Feature vector 
$x_m^i$ for movie $i$
* task is to find out if a movie $i$ is a good match for user $j$
* match users and movies
* define $v_u^j$ is a vector of numbers computed from features $x_u^j$ of 
user $j$, e.g. preferences for certain genres
* define $v_m^i$ is a vector of numbers computed from features $x_m^i$ of 
movie $i$, e.g. degree of match with a genre
* calculate dot product of $v_m^j * v_u^j$ for a new movie to find out if it is 
a good match.

User network and movie network:

* input: user features $x_u$ (user network) and movie features (feature network)
* output: vectors $v_u$ and $v_m$
* cost function: $J=\Sigma_{(i,j):r(i,j)=1}(v_u^j*v_m^i-y^{(i, j)}))^2+regularization$
* $J$ will be optimized for users who gave ratings for certain movies

Effective calculation:

* retrieval step: 
  * generate large list of plausible item candidates, e.g. similar 
  movies to the last 10 movies watched by a user, top 10 movies of the most 
  viewed genre, top movies in the users county
  * combine retrieved items in a list, de-duplicate, remove items already 
  watched/purchased
* ranking step: rank retrieved list using the learned model, i.e. feed it into 
the network and make prediction on user rating
* offline experiments can help to determine the size of the retrieval list, i.e. 
investigate if finally more relevant recommendations are added to the final list





## Reinforcement learning

