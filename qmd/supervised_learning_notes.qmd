---
title: "Notes supervised learning"
format: html
editor: source
editor_options: 
  chunk_output_type: console
engine: knitr
---

```{r}
knitr::opts_knit$set(
  root.dir = '~/Documents/repos/github/machinelearning'
)
```


## Linear regression

### Gradient descent

* random starting parameters
* select loss function (e.g. MSE, RMSE, ...)
* loss is calculated per observation, sum of all losses gives the cost $J$
* change parameters to reduce cost $J$ to possible (local) minimum
* example of gradient descent for cost function $J(w, b)$:
* for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)$ 
* for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
* with $\alpha$: learning rate, e.g. 0.01, corresponds to the step size
* derivative indicates direction and grade of descent per parameter
* in case of one parameter, partial derivative is tangent line
* in case of two parameters, partial derivatives form a 2D-plane 
* parameters are updated simultaneously
* near local minimum update steps get smaller by design, even with constant 
learning rate $\alpha$
* repeat until convergence, i.e. changes get smaller per step
* small leaning rate leads to slow descent (many steps)
* large leaning rate may miss the minimum and fail to converge
* stopping criteria:
* derivative of cost function is close to zero
* small changes of parameters 

Example with linear regression:

* partial derivatives
* always one global minimum
* in linear regression **batch** gradient descent is used which uses the entire 
training data at each step

In linear regression parameters $w$ can be estimated using the normal equation 
without iterations. This is usually impossible in other models. 


### Feature scaling

Features varying largely in scale will cause parameters $w$ to also be very 
different. This can make gradient descent inefficient as the hyperplane gets 
narrow in one direction. 

Methods of scaling:  

* normalize to maximum: x / max(x)
* normalize to mean and range: (x - mean(x)) / range(x)
* normalize to mean and sd: (x - mean(x)) / sd(x)


### Convergence of gradient descent

* plot cost function $J$ against number of iterations
* ideally $J$ should decrease on every iteration and at some stage flatten out 
(convergence)
* $J$ should never increase as iterations increase, i.e. no bumps, no steady 
increase
* number of iterations will vary based on data and model
* declaration of convergence by setting a minimum $\epsilon$, which sets a 
threshold to the decrease of $J$ for one more iteration. I.e. if $J$ is 
decreasing with less than $\epsilon$, convergence is assumed. 


### Learning rate

* small $\alpha$: many iterations
* large $\alpha$: overshooting, i.e. no convergence (ups and downs in the cost 
function plot)
* good practice: test different learning rates (0.001, 0.003, 0.01, 0.03, 0.1, ...) 
with a small number of iterations and plot cost function versus iterations, e.g. 


### Feature engineering

* transforming or combining original features using intuition, prior knowledge
* polynomial regression to create features from squares or cubes
* feature scaling may become even more important on combined features


## Classification with logistic regression

* sigmoid function: $g(z)=\frac{1}{1+e^{-z}}$
* $g(z)$ is between 0 and 1
* logistic regression applies the sigmoid function to the linear regression 
through the linear predictor $z=wx+b$
* output means $P(y=1|x;w,b)$

```{r}
#| label: sigmoid function

z = seq.int(-5L, to = 5L, length.out = 100)
g = 1 / (1 + exp(-z))
plot(z, g, type = "l")
abline(v = 0, lty = 2)
abline(h = 0.5, lty = 2)
```


### Decision boundary

* decision boundary is given for $z=0$ or $wx+b=0$ (univariate)
* using a decision boundary of 0.5 for the sigmoid function corresponds to 
negative values of $z$ being mapped to 0 and positive values of $z$ being 
mapped to 1
* non-linear decision boundaries can be identified e.g. with polynomials


### Loss and cost function

* Squared error loss function is not suitable for logistic regression 
* Define loss function $L$ per observation as

$$
L(f_{w, b}(x^i), y^i)=
\begin{cases}
-log(f_{w, b}(x^i)), \text{ if } y^i = 1 \\
-log(1-f_{w, b}(x^i)), \text{ if } y^i = 0 
\end{cases}
$$
* reminder: loss is per observation, sum of loss is cost $J$
* cost $J$ for a certain set of parameters is 
$J(w,b)=\frac{1}{m}\Sigma_1^m L(f_{w, b}(x^i), y^i)$

```{r}
#| label: loss-ex-r

f = seq(0, to = 1, by = 0.01)
loss_1 = -log(f) 
loss_0 = -log(1-f) 

plot(
  f
  , y = loss_1
  , type = "l"
  , col = "darkblue"
  , xlab = "prediction"
  , ylab = "loss (blue: y is 1, red: y is 0)"
)

lines(f, y = loss_0, col = "darkred")
```

```{python}
#| label: loss-ex-py

import numpy as np
import matplotlib.pyplot as plt

f = np.arange(0.01, stop=1, step=0.01)
loss_1 = -np.log(f)
loss_0 = -np.log(1 - f)

plt.plot(f, loss_1, c='darkblue')
plt.plot(f, loss_0, c='darkred')
plt.xlabel('prediction')
plt.ylabel('loss (blue: y is 1, red: y is 0)')
plt.show()
```


### Loss and cost function simplified

* Given that outputs can only be 0 or 1, the loss function $L$ per observation 
can be rewritten as

$$
L(f_{w, b}(x^i), y^i)=-y^ilog(f_{w, b}(x^i))-(1-y^i)log(1-f_{w, b}(x^i))
$$

* if $y^i=1$ this simplifies to $L(f_{w, b}(x^i), y^i)=-log(f_{w, b}(x^i))$
* if $y^i=0$ this simplifies to $L(f_{w, b}(x^i), y^i)=-log(1-f_{w, b}(x^i))$

* Cost can be rewritten as

$$
J(w,b)=-\frac{1}{m}\Sigma_1^m[y^ilog(f_{w, b}(x^i))+(1-y^i)log(1-f_{w, b}(x^i))]
$$

* Rationale behind this cost function is maximum likelihood


### Gradient descent for logistic regression

* Derivatives

  * for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)$ 
  * for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
  
* simultaneous updates of $w$ and $b$
* note that $f_{w, b}$ is the sigomoid function in logistic regression


## Overfitting and underfitting

* poor fit to training data: underfitting, means high bias, low variance
* high fit to training data: overfitting, means low bias, high variance
* well fit to training data: good generalization, balance between bias and variance


## Handling overfitting

* causes for overfitting: low number of samples, high number of features, 
complex model

Adressing overfitting:

* collect more training examples (difficult)
* select subset of features (risk of information loss)
* regularization: shrink parameter values w/o setting it to exactly to 0
* practice hint: regularize $w$, but not $b$ (should not have impact)


## Regularization

Cost function:

* known cost function 
$J(w,b)=\frac{1}{2m}\Sigma^m_{1}(f_{w,b}(x^i)-y^i))^2$
* regularized cost function 
$J(w,b)=\frac{1}{2m}\Sigma^m_{i=1}(f_{w,b}(x^i)-y^i))^2+\frac{\lambda}{2m}\Sigma_{j=1}^nw^2_j$
* second term is the regularization term 
* $\lambda$ is the regularization parameter

Linear regression:

Cost function:

$$
J(w,b)=\frac{1}{2m}\Sigma_{i=1}^m(f_{w, b}(x^i)-y^i)+\frac{\lambda}{2m}\Sigma_{j=1}^nw_j
$$

* Derivatives

  * for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)=\frac{1}{m}\Sigma^m_{i=1}(f_{w,b}(x^i)-y^i)x^i+\frac{\lambda}{m}w_j$ 
  * for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
  * $b$ remains the same as we do not regularize it


Logistic regression:

Cost function:

$$
J(w,b)=-\frac{1}{m}\Sigma_{i=1}^m[y^ilog(f_{w, b}(x^i))+(1-y^i)log(1-f_{w, b}(x^i))]+\frac{\lambda}{2m}\Sigma_{j=1}^nw_j
$$

* Derivatives

  * for parameter $w$: $w=w-\alpha\frac{\partial}{\partial w}J(w, b)=\frac{1}{m}\Sigma^m_{i=1}(f_{w,b}(x^i)-y^i)x^i+\frac{\lambda}{m}w_j$ 
  * for parameter $b$: $B=b-\alpha\frac{\partial}{\partial b}J(w, b)$
  * $b$ remains the same as we do not regularize it

