---
title: "Notes advanced learning algorithms"
format: html
editor: source
editor_options: 
  chunk_output_type: console
engine: knitr
---

```{r}
knitr::opts_knit$set(
  root.dir = '~/Documents/repos/github/machinelearning'
)
```


## Neural networks

* neural networks = deep learning algorithms
* inference: take a pre-trained NN and make predictions
* traditional algorithms (linear, logistic regression) do not scale with amount of data
* activation function takes input and creates output, e.g. by logistic regression
* layer: group of n neurons (1..n), connected to the same input features
* output layer: final layer delivering the output
* input layer: first layer with features
* hidden layer: combines inputs from earlier layers to new features
* input $x$, activation $a$
* activations: logistic regressions that generate new features
* activations: higher level features
* architecture: number of hidden layers and hidden units per layer
* "multilayer perceptron": NN with multiple hidden layers


### Notation

* $a^l=g(w^l_j*a^{l-1}+b^l_j)$
* $a$ is the activation
* $l$ is the layer
* $j$ is the unit / neuron

* inference = making predictions, aka forward propagation


### Efficient implementation

* dot product
* matrix notation, transpose
* matrix multiplication
* in Python `np.matmul` or `@`


### Predicting from a NN in tensorflow

* create the model, i.e. define layers (architecture)
* load pre-calculated weights
* run prediction


### Training a NN in tensorflow

* create the model, i.e. define layers (architecture)
* define cost function, e.g. binary crossentropy
* train with a max number of epochs, i.e. minimize cost function $J(w,b)$
* training = back-propagation


### Binary crossentropy loss

* binary classification problem
* same as logistic loss $L(f(x),y)=-ylog(f(x))-(1-y)log(1-f(x))$
* other loss function in tensorflow: mean squared error (regression problem)


### Activation functions

* linear activation $g(z)=z$ (aka no activation)
* sigmoid activation $g(z)=\frac{1}{1+e^{-z}}$
* rectify linear unit `ReLU` function $g(z) = max(0, z)$
* softmax activation 

Choosing activation functions:

* output layer:

  * binary prediction: sigmoid
  * regression: linear activation
  * non-negative predictions: ReLU
  * multi-class ouptut: softmax
  
* hidden layers:

  * most common activation today is ReLU 
  * ReLU is faster than sigmoid
  * cost function for sigmoid is harder to minimize 
  
Need of activation functions:

* output: linear, hidden: linear: simplifies to linear regression
* output: sigmoid, hidden: linear: simplifies to logistic regression
* linear function of linear function is also a linear function
* never use linear activation in hidden layers


### Multiclass classification

* MNIST example 
* softmax regression
* example of four outputs (1..4):

  * calculate $z_1, z_2, z_3, z_4$, e.g. $z_1=w_1*x+b_1$
  * calculate $a_1, a_2, a_3, a_4$, e.g. $a_1=\frac{e^{z_1}}{\Sigma_{i=1}^4 e^{z_i}}$
  * general form: $z_j=w_j*x+b$, followed by $a_j=\frac{e^{z_j}}{\Sigma_{k=1}^j e^{z_k}}$
  * with $a_j=P(y=j|x)$ 
  
* loss function (crossentropy loss):

$$
L(a_1, ..., a_N, y)=
\begin{cases}
-log(a_1), \text{ if } y = 1 \\
-log(a_2), \text{ if } y = 2 \\
... \\
-log(a_N), \text{ if } y = N \\
\end{cases}
$$
* architecture of softmax with n classes requires output layer with n units
* softmax is unique as output in one output node depends on output of all other 
output nodes
* loss function is called `SparseCategoricalCrossentropy` in tensorflow 
* use argument `from_logits=True` when defining loss for dealing with roundup errors
* this requires the output layer to use linear activation (i.e. returning $z$) and calculate
probabilities afterwards using the sigmoid function ($g(z)$)


### Multilabel classification

* frequently used in image analysis
* multiple labels per image (e.g. car, bus, pedestrian, building)
* possible approaches:

  * define multiple networks, one per label
  * define one network with three outputs (output layer with three units) and 
  sigmoid activations
  
  
### Advanced optimization

* "Adam algorithm", adjusts $\alpha$ automatically to be smaller or larger
* Adam = Adaptive Moment estimation
* also $\alpha$ is different per $w$, i.e. $\alpha_j$
* only requires an initial learning rate
* Adam is more robust to learning rate
* de facto standard in NN training


### Additional layer types

* dense layer     
  
  * all activations from the previous layer are submitted to every neuron
  * i.e. every neurons output is a function of all activations 
  
* convolutional layer 

  * only a subset of activations from the previous layer is submitted to each neuron
  * i.e. each neurons output is a function of a subset of activations 
  * faster computation
  * need less training data
  * many choices regarding architecture (size of subsets)


### Back propagation 

* back propagation is an algorithm used to efficiently calculate derivatives
* computation graphs are used to simplify the operation 


## Model evaluation

* evaluate model performance by train-test-split
* compare train error $J$ with test error
* calculate cost $J_{train}$ for train and $J_{test}$ for test set (regression, 
classification)
* calculate fraction of misclassified observations in train and test 
(classification)
* cross-validation approach: split data into training, cross-validation (aka 
validation set or development set or dev set) and test set
* cross-validation approach is recommended if additional parameters are 
optimized (e.g. degree of a polynomial, number of layers in a NN, ...)
* error is same as cost except the regularization term
* test set is only used for estimating the generalization error



